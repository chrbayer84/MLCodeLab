{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Packages\n",
    "In this tutorial, we'll take you through developing models to classify images in PyTorch from start to finish. We'll go through preprocessing, building neural networks, and experimentation.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch\n",
    "It's a python based deep learning library. It's very popular amongst researchers because of it's speed and flexibility. \n",
    "\n",
    "At the base of pytorch is the idea of a `Tensor`.\n",
    "A `Tensor` is just an `n-dimensional` array, like a numpy `ndarray`.\n",
    "\n",
    "For example,\n",
    "Let's make a random `3x3` tensor.\n",
    "We can inspect tensors by printing them, and get their size with `.size()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2503,  0.3767,  0.4267],\n",
      "        [ 0.0050,  0.5662,  0.3950],\n",
      "        [ 0.4139,  0.4305,  0.9178]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take an array, and convert it to a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "b = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "Any operation between tensors produces new tensors.\n",
    "You can use regular python syntax to add, multiply them. PyTorch also nice functions for matrix multipication, and reshaping tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.2503,  4.3767,  4.4267],\n",
      "        [ 4.0050,  4.5662,  4.3950],\n",
      "        [ 4.4139,  4.4305,  4.9178]])\n",
      "tensor([[ 8.5007,  8.7535,  8.8534],\n",
      "        [ 8.0099,  9.1324,  8.7901],\n",
      "        [ 8.8279,  8.8609,  9.8356]])\n",
      "tensor([[-4.2503, -4.3767, -4.4267],\n",
      "        [-4.0050, -4.5662, -4.3950],\n",
      "        [-4.4139, -4.4305, -4.9178]])\n",
      "torch.Size([3, 3]) torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = a + 4\n",
    "print(a)\n",
    "d = a * 2\n",
    "print(d)\n",
    "e = a - d\n",
    "print(e)\n",
    "\n",
    "print(a.size(), b.size())\n",
    "# Wont work because shapes number of columns in a \n",
    "# doesn't match number of rows in b\n",
    "'c = torch.matmul(a, b)'\n",
    "# This will work\n",
    "c = torch.matmul(b, a)\n",
    "print(c.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running into a bug, it's often helpful to step through and check your dimensions.\n",
    "\n",
    "### The magic: Autograd\n",
    "The power behind PyTorch comes from its automatic differentiation engine, Autograd. To turn it on, construct your tensors with `requires_grad = True`.\n",
    "\n",
    "Every computation you make, i.e `c=a+b` will create a computation graph with node `c` being linked to `a` and `b` via a `+` operator. \n",
    "\n",
    "<img src=\"compute_graph.png\">\n",
    "\n",
    "If you call `.backward()` on your final node, autograd will work out all the gradients for you and store the values in `a.grad` and `b.grad`.\n",
    "\n",
    "Let's look at an example.\n",
    "\n",
    "Consider the function\n",
    "`y = a*(x^2) + b`, where `a = b = 1`. This is a simple parabola. \n",
    "<img src=\"parabola.png\">\n",
    "\n",
    "The compute graph for this would be:\n",
    "<img src=\"compute_graph_parabola.png\">\n",
    "\n",
    "From basic calculas, we know that the derivative of `dy/dx` is\n",
    "`dy/dx = 2a x`. So the derivative at `x = 1` is `2`. \n",
    "This wasn't very hard, but let's see how autograd can do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.])\n",
      "x.grad=tensor([ 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1, requires_grad=False)\n",
    "b = torch.ones(1, requires_grad=False)\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "y = a*(x*x)  + b\n",
    "print(y)\n",
    "y.backward()\n",
    "print(\"x.grad={}\".format(x.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why autograd is exciting\n",
    "Now, this may have seemed trivial and contrived, but this flexible automatic differentiation process really shines when our computation graph is large and complex, i.e when it's a neural network.\n",
    "\n",
    "If we place our whole model into our computation graph, and the loss calculation, the a call to `backward`, will compute all the gradients, and it becomes very easy to train neural networks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Task: MNIST, Digit Classification\n",
    "<img src=\"mnist.png\">\n",
    "\n",
    "In this lab, we'll build a neural network to classify hand-written digits.\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Loading Data and Preprocessing\n",
    "Let's start by loading the data.\n",
    "We're going to normalize our images to have 0 mean, and unit variance. We'll do this using some torchvision transforms. This generally helps stablize learning, and is common practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_image = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])\n",
    "\n",
    "all_train = datasets.MNIST('data', train=True, download=True,\n",
    "                        transform=normalize_image)\n",
    "num_train = int(len(all_train)*.8)\n",
    "train = [all_train[i] for i in range(num_train)]\n",
    "dev = [all_train[i] for i in range(num_train,len(all_train))]\n",
    "test = datasets.MNIST('data', train=False, download=True, \n",
    "                      transform=normalize_image)\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building a model\n",
    "\n",
    "All pytorch models should be implemented as instances of `nn.Module`. \n",
    "\n",
    "To build a model you need to:\n",
    "a) define what parameters it'll need in it's `__init__` function\n",
    "b) define the model's computation, using those parameters, in a forward function.\n",
    "\n",
    "\n",
    "To keep things simple, lets define a simple linear classifer, like logistic regression. We'll experiment with more complex models soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Defining our training procedure\n",
    "\n",
    "To train our model, let's introduce a couple new PyTorch ideas.\n",
    "\n",
    "A [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is an iterator that goes over our entire dataset and selects batches. \n",
    "We'll be using this to iterate through our train/dev/test sets.\n",
    "\n",
    "Let's intialize these now. \n",
    "\n",
    "An [Optimizer](https://pytorch.org/docs/stable/optim.html) defines an update rule. In class, we've discussed vanilla SGD, which is one method to compute the next weight, given the current weight and gradient. There are plently of other optimizers you can try from the pytorch library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model:\n",
    "\n",
    "1) we'll randomly sample batches from our train loader\n",
    "\n",
    "2) compute our loss (using standard `cross_entropy`)\n",
    "\n",
    "3) compute our gradients (by calling `backward()` on our loss)\n",
    "\n",
    "4) update our neural network with an `optimizer.step()`, and go back to 1)\n",
    "\n",
    "I've added some extra stuff here to log our accuracy and avg loss for the epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch( model, train_loader, optimizer, epoch):\n",
    "    model.train() # Set the nn.Module to train mode. \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    num_samples = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total_loss += loss.detach() # Don't keep computation graph \n",
    "\n",
    "    print('Train Epoch: {} \\tLoss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            epoch, total_loss / num_samples, \n",
    "            correct, \n",
    "            num_samples,\n",
    "            100. * correct / num_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5 Define our evaluation loop\n",
    "Similar to above, we'll also loop through our dev or test set, and compute our loss and accuracy. \n",
    "This lets us see how well our model is generalizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, test_loader, name):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        name,\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_epoch(model, train_loader, optimizer, epoch)\n",
    "    eval_epoch(model,  dev_loader, \"Dev\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Experiment with MLP\n",
    "This model gets a dev accuracy of 93%, which isn't too bad. However, the power of neural networks comes from composing layers with nonlinearities.\n",
    "\n",
    "Let's try a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \tLoss: 0.0098, Accuracy: 40031/48000 (83%)\n",
      "\n",
      "Dev set: Average loss: 0.0045, Accuracy: 11007/12000 (92%)\n",
      "\n",
      "---\n",
      "Train Epoch: 2 \tLoss: 0.0042, Accuracy: 44285/48000 (92%)\n",
      "\n",
      "Dev set: Average loss: 0.0034, Accuracy: 11245/12000 (94%)\n",
      "\n",
      "---\n",
      "Train Epoch: 3 \tLoss: 0.0032, Accuracy: 45093/48000 (94%)\n",
      "\n",
      "Dev set: Average loss: 0.0029, Accuracy: 11373/12000 (95%)\n",
      "\n",
      "---\n",
      "Train Epoch: 4 \tLoss: 0.0026, Accuracy: 45721/48000 (95%)\n",
      "\n",
      "Dev set: Average loss: 0.0024, Accuracy: 11489/12000 (96%)\n",
      "\n",
      "---\n",
      "Train Epoch: 5 \tLoss: 0.0021, Accuracy: 46090/48000 (96%)\n",
      "\n",
      "Dev set: Average loss: 0.0021, Accuracy: 11552/12000 (96%)\n",
      "\n",
      "---\n",
      "Train Epoch: 6 \tLoss: 0.0018, Accuracy: 46409/48000 (97%)\n",
      "\n",
      "Dev set: Average loss: 0.0019, Accuracy: 11594/12000 (97%)\n",
      "\n",
      "---\n",
      "Train Epoch: 7 \tLoss: 0.0015, Accuracy: 46641/48000 (97%)\n",
      "\n",
      "Dev set: Average loss: 0.0017, Accuracy: 11618/12000 (97%)\n",
      "\n",
      "---\n",
      "Train Epoch: 8 \tLoss: 0.0013, Accuracy: 46810/48000 (98%)\n",
      "\n",
      "Dev set: Average loss: 0.0016, Accuracy: 11662/12000 (97%)\n",
      "\n",
      "---\n",
      "Train Epoch: 9 \tLoss: 0.0012, Accuracy: 46988/48000 (98%)\n",
      "\n",
      "Dev set: Average loss: 0.0015, Accuracy: 11679/12000 (97%)\n",
      "\n",
      "---\n",
      "Train Epoch: 10 \tLoss: 0.0010, Accuracy: 47089/48000 (98%)\n",
      "\n",
      "Dev set: Average loss: 0.0014, Accuracy: 11692/12000 (97%)\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        hidden = F.relu(self.fc2(hidden))\n",
    "        logit = self.fc3(hidden)\n",
    "        return logit\n",
    "    \n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_epoch(model, train_loader, optimizer, epoch)\n",
    "    eval_epoch(model,  dev_loader, \"Dev\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Experiment with CNN\n",
    "A 3 layer MLP gets a dev accuracy of 97%, a strong imporvement over the simple linear model.\n",
    "Now let's experiment with a covolutional neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = 128\n",
    "        self.conv1 = nn.Conv2d(1, self.hidden_dim, kernel_size=3)\n",
    "        self.fc = nn.Linear(self.hidden_dim, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "        \n",
    "        hidden = F.relu(self.conv1(x))\n",
    "        hidden = hidden.view((batch_size, self.hidden_dim, -1))\n",
    "        hidden,_ = torch.max(hidden, dim=-1)\n",
    "        logit = self.fc(hidden)\n",
    "        return logit\n",
    "    \n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_epoch(model, train_loader, optimizer, epoch)\n",
    "    eval_epoch(model,  dev_loader, \"Dev\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Explore further.\n",
    "You can try different model architectures, different optimizers, learning rates and regularization strategies. Neural networks are incredibly flexibile, and so the space to do explore is enourmous.  Once you're done exploring, take your best model (i.e achieves best results on dev set) and run it on test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9740/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_epoch(model,  test_loader, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Now try it on your own on CIFAR\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
